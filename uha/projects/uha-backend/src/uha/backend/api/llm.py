"""LLM integration API endpoints."""

from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from uha.backend.container import ApplicationContainer
from uha.backend.database.models import DatabaseManager
from uha.backend.models.stream_models import (
    LiveStreamEntry,
    PaginatedStreamsRequest,
    PaginatedStreamsResponse,
    StreamWithDetails,
)
from uha.backend.services.cache_service import StreamCacheService
from uha.backend.settings import Settings

router = APIRouter(prefix="/llm", tags=["LLM"])

# Initialize database and cache service
db_manager = DatabaseManager()
cache_service = StreamCacheService(db_manager)


class SummaryRequest(BaseModel):
    content: str
    max_tokens: Optional[int] = 500
    temperature: Optional[float] = 0.7


class SummaryResponse(BaseModel):
    summary: str
    original_length: int
    summary_length: int


class LiveStreamSummaryRequest(BaseModel):
    year: int
    date_filter: Optional[str] = None  # YYYY-MM-DD format
    include_detailed_analysis: bool = False  # YouTube Data API ìƒì„¸ ë¶„ì„ í¬í•¨ ì—¬ë¶€
    max_videos_to_analyze: int = 20  # ë¶„ì„í•  ìµœëŒ€ ì˜ìƒ ìˆ˜


class LiveStreamSummaryResponse(BaseModel):
    entries: List[LiveStreamEntry]
    summary: str
    total_streams: int
    detailed_analysis: Optional[Dict[str, Any]] = None  # YouTube Data API ë¶„ì„ ê²°ê³¼
    common_keywords: Optional[List[str]] = None  # ê³µí†µ í‚¤ì›Œë“œ
    engagement_stats: Optional[Dict[str, int]] = None  # ì°¸ì—¬ë„ í†µê³„


async def get_settings() -> Settings:
    """Get application settings."""
    container = ApplicationContainer()
    return container.settings.provided()


async def call_lm_studio(prompt: str, max_tokens: int = 500, temperature: float = 0.3) -> str:
    """Call LM Studio API for text generation."""
    url = "http://localhost:1234/v1/chat/completions"

    payload = {
        "model": "qwen/qwen3-4b",
        "messages": [
            {
                "role": "system",
                "content": "You must respond ONLY in Korean. ë‹¹ì‹ ì€ í•œêµ­ì–´ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´ë‚˜ ë‹¤ë¥¸ ì–¸ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ì…ë‹ˆë‹¤. 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            },
            {"role": "user", "content": prompt},
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": ["\n\n", "ìš”ì•½:", "Summary:"],
    }

    try:
        print(f"LM Studio í˜¸ì¶œ ì¤€ë¹„ ì¤‘... URL: {url}")
        print(f"í˜ì´ë¡œë“œ: {payload}")

        async with httpx.AsyncClient(timeout=30.0) as client:
            print("HTTP ìš”ì²­ ì‹œì‘...")
            response = await client.post(url, json=payload)
            print(f"ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")

        if response.status_code != 200:
            print(f"ì‘ë‹µ ë‚´ìš©: {response.text}")
            raise HTTPException(status_code=500, detail=f"LM Studio API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")

        data = response.json()
        print(f"LM Studio ì‘ë‹µ ë°ì´í„°: {data}")
        content = data["choices"][0]["message"]["content"]

        # Clean up AI response - remove thinking tags and extra content
        content = content.strip()
        if content.startswith("<think>"):
            # Find the end of thinking section and extract only the actual response
            think_end = content.find("</think>")
            if think_end != -1:
                content = content[think_end + 8 :].strip()

        # Remove any remaining XML-like tags
        import re

        content = re.sub(r"<[^>]+>", "", content).strip()

        # Take only the first few sentences for summary
        sentences = content.split(".")
        if len(sentences) > 4:
            content = ". ".join(sentences[:4]) + "."

        return content

    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LM Studio API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail="LM Studio ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. http://localhost:1234 ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LM Studio API ì˜¤ë¥˜: {str(e)}")


async def fetch_live_stream_data(year: int) -> str:
    """Fetch live stream data from GitHub repository."""
    url = f"https://raw.githubusercontent.com/eun2ce/uzuhama-live-link/main/readme-{year}.md"

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url)

        if response.status_code != 200:
            raise HTTPException(status_code=404, detail=f"{year}ë…„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        return response.text

    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="GitHub ì €ì¥ì†Œì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(e)}")


def parse_live_stream_data(content: str, date_filter: Optional[str] = None) -> List[LiveStreamEntry]:
    """Parse live stream data from markdown content."""
    entries = []
    lines = content.strip().split("\n")

    for line in lines:
        line = line.strip()

        # Skip header lines and separators
        if line.startswith("|") and "Date" not in line and "---" not in line:
            # Parse markdown table format: | Date | URL |
            parts = [part.strip() for part in line.split("|") if part.strip()]
            if len(parts) >= 2:
                date = parts[0].strip()
                url_part = parts[1].strip()

                # Extract URL from markdown link format [text](url)
                if url_part.startswith("[") and "](" in url_part and url_part.endswith(")"):
                    url = url_part.split("](")[1][:-1]  # Extract URL from [text](url)
                else:
                    url = url_part

                # Apply date filter if provided
                if date_filter and not date.startswith(date_filter):
                    continue

                entries.append(LiveStreamEntry(date=date, url=url))

        # Also handle tab-separated format for backward compatibility
        elif "\t" in line:
            parts = line.split("\t")
            if len(parts) >= 2:
                date = parts[0].strip()
                url = parts[1].strip()

                # Apply date filter if provided
                if date_filter and not date.startswith(date_filter):
                    continue

                entries.append(LiveStreamEntry(date=date, url=url))

    return entries


def extract_video_id_from_url(url: str) -> str:
    """Extract video ID from YouTube URL."""
    import re

    patterns = [
        r"(?:youtube\.com\/watch\?v=|youtu\.be\/)([a-zA-Z0-9_-]{11})",
        r"youtube\.com\/embed\/([a-zA-Z0-9_-]{11})",
        r"youtube\.com\/v\/([a-zA-Z0-9_-]{11})",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return ""


async def get_stream_details(entry: LiveStreamEntry, settings: Settings) -> StreamWithDetails:
    """Get detailed information for a single stream with caching."""
    print(f"ìŠ¤íŠ¸ë¦¼ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹œì‘: {entry.url}")
    video_id = extract_video_id_from_url(entry.url)
    print(f"ì¶”ì¶œëœ ë¹„ë””ì˜¤ ID: {video_id}")

    if not video_id:
        print(f"ë¹„ë””ì˜¤ ID ì—†ìŒ: {entry.url}")
        return StreamWithDetails(
            date=entry.date,
            url=entry.url,
            video_id=video_id,
        )

    # Try to get from cache first
    cached_stream = await cache_service.get_cached_stream(video_id)
    if cached_stream:
        print(f"ìºì‹œì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ë¡œë“œ: {video_id}")
        return cached_stream

    print(f"ìºì‹œì— ì—†ìŒ, ìƒˆë¡œ ì²˜ë¦¬: {video_id}")

    stream_details = StreamWithDetails(
        date=entry.date,
        url=entry.url,
        video_id=video_id,
    )

    print(
        f"YouTube API í‚¤ ìƒíƒœ: {bool(settings.youtube_api_key)} (ê¸¸ì´: {len(settings.youtube_api_key) if settings.youtube_api_key else 0})"
    )

    if not settings.youtube_api_key:
        print("YouTube API í‚¤ ì—†ìŒ - ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸")
        # ë”ë¯¸ ë°ì´í„°ë¡œ AI ìš”ì•½ í…ŒìŠ¤íŠ¸
        stream_details.title = f"í…ŒìŠ¤íŠ¸ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ - {entry.date}"
        # YouTube ì¸ë„¤ì¼ URL íŒ¨í„´ ì‚¬ìš©
        stream_details.thumbnail = f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
        stream_details.ai_summary = await generate_stream_summary(
            stream_details.title,
            "í…ŒìŠ¤íŠ¸ìš© ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì…ë‹ˆë‹¤. ì‹œì²­ìë“¤ê³¼ í•¨ê»˜ ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤.",
            ["ì¬ë°Œì–´ìš”!", "ëŒ€ë°•!", "ë‹¤ìŒì—ë„ í•´ì£¼ì„¸ìš”"],
            ["ê²Œì„", "ë¼ì´ë¸Œ"],
            ["ìŠ¤íŠ¸ë¦¬ë°", "ì¬ë¯¸"],
        )
        stream_details.highlights = ["ğŸ® ê²Œì„ ìŠ¤íŠ¸ë¦¬ë°", "ğŸ’¬ ì‹œì²­ìì™€ ì†Œí†µ"]
        stream_details.sentiment = "ê¸ì •ì ì¸ ë°˜ì‘ì´ ë§ì€ ìŠ¤íŠ¸ë¦¼"
        stream_details.engagement_score = 7.5
        stream_details.category = "ğŸ® ê²Œì„"
        stream_details.view_count = 1500
        stream_details.like_count = 120
        stream_details.comment_count = 45
        stream_details.duration = "PT1H30M"

        # Cache the dummy data
        await cache_service.cache_stream(stream_details)
        return stream_details

    try:
        # YouTube Data API í˜¸ì¶œ
        from uha.backend.api.youtube_analysis import extract_keywords_from_text, get_video_comments, get_video_details

        video_data = await get_video_details(video_id, settings.youtube_api_key)
        snippet = video_data["snippet"]
        statistics = video_data["statistics"]
        content_details = video_data["contentDetails"]

        # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
        comments_data = await get_video_comments(video_id, settings.youtube_api_key, max_results=20)
        comment_texts = []
        for comment_item in comments_data:
            comment = comment_item["snippet"]["topLevelComment"]["snippet"]
            comment_texts.append(comment["textDisplay"])

        # ì¸ë„¤ì¼ URL
        thumbnails = snippet.get("thumbnails", {})
        thumbnail_url = (
            thumbnails.get("maxres", {}).get("url")
            or thumbnails.get("high", {}).get("url")
            or thumbnails.get("medium", {}).get("url")
            or thumbnails.get("default", {}).get("url")
        )

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        text_for_keywords = f"{snippet['title']} {snippet.get('description', '')}"
        keywords = extract_keywords_from_text(text_for_keywords, max_keywords=5)

        # ê¸°ë³¸ ì •ë³´ ì„¤ì •
        stream_details.title = snippet["title"]
        stream_details.thumbnail = thumbnail_url
        stream_details.view_count = int(statistics.get("viewCount", 0))
        stream_details.like_count = int(statistics.get("likeCount", 0))
        stream_details.comment_count = int(statistics.get("commentCount", 0))
        stream_details.duration = content_details.get("duration", "")
        stream_details.tags = snippet.get("tags", [])[:5]  # ìµœëŒ€ 5ê°œ íƒœê·¸
        stream_details.keywords = keywords

        # ì¶”ê°€ ë¶„ì„ ì •ë³´ ìƒì„±
        try:
            print(f"AI ë¶„ì„ ì‹œì‘: {video_id} - {snippet['title']}")
            # AI ìš”ì•½ ìƒì„±
            stream_details.ai_summary = await generate_stream_summary(
                snippet["title"], snippet.get("description", ""), comment_texts, stream_details.tags or [], keywords
            )
            print(f"AI ìš”ì•½ ì™„ë£Œ: {stream_details.ai_summary[:50]}...")

            # í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
            stream_details.highlights = extract_highlights_from_comments(comment_texts, snippet["title"])

            # ê°ì • ë¶„ì„
            from uha.backend.api.youtube_analysis import analyze_video_sentiment

            stream_details.sentiment = await analyze_video_sentiment(
                snippet["title"], snippet.get("description", ""), comment_texts
            )

            # ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
            duration_minutes = parse_duration_to_minutes(stream_details.duration)
            stream_details.engagement_score = calculate_engagement_score(
                stream_details.view_count, stream_details.like_count, stream_details.comment_count, duration_minutes
            )

            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            stream_details.category = categorize_stream(snippet["title"], stream_details.tags or [], keywords)

        except Exception as analysis_error:
            print(f"Error in additional analysis for {video_id}: {str(analysis_error)}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            stream_details.ai_summary = f"{snippet['title']}ì—ì„œ ì§„í–‰ëœ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°ì…ë‹ˆë‹¤."
            stream_details.highlights = ["ğŸ“º ë¼ì´ë¸Œ ë°©ì†¡"]
            stream_details.sentiment = "ì¤‘ë¦½ì ì¸ ë°˜ì‘ì˜ ìŠ¤íŠ¸ë¦¼"
            stream_details.engagement_score = 0.0
            stream_details.category = "ğŸ“º ì¼ë°˜"

    except Exception as e:
        print(f"Error getting details for {entry.url}: {str(e)}")

    # Cache the processed stream data
    await cache_service.cache_stream(stream_details)
    return stream_details


async def generate_stream_summary(
    title: str, description: str, comments: List[str], tags: List[str], keywords: List[str]
) -> str:
    """ê°œë³„ ìŠ¤íŠ¸ë¦¼ì— ëŒ€í•œ AI ìš”ì•½ ìƒì„±."""
    try:
        print(f"ìš”ì•½ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ: {title}")

        # ëŒ“ê¸€ì—ì„œ ì£¼ìš” ë‚´ìš© ì¶”ì¶œ (ìƒìœ„ 10ê°œ)
        top_comments = comments[:10]
        comments_text = " ".join(top_comments) if top_comments else ""

        # íƒœê·¸ì™€ í‚¤ì›Œë“œ ê²°í•©
        all_tags = ", ".join(tags + keywords) if tags or keywords else ""

        prompt = f"""
ë‹¤ìŒ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ 2-3ë¬¸ì¥ì˜ ê°„ê²°í•œ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ì„¤ëª…: {description[:200]}...
ì£¼ìš” íƒœê·¸/í‚¤ì›Œë“œ: {all_tags}
ì‹œì²­ì ëŒ“ê¸€ ìš”ì•½: {comments_text[:300]}...

ìš”ì•½ ì¡°ê±´:
1. ìŠ¤íŠ¸ë¦¼ì˜ ì£¼ìš” ë‚´ìš©ê³¼ íŠ¹ì§•ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…
2. ì‹œì²­ìë“¤ì˜ ë°˜ì‘ì´ë‚˜ í•˜ì´ë¼ì´íŠ¸ê°€ ìˆë‹¤ë©´ í¬í•¨
3. í•œêµ­ì–´ë¡œë§Œ ì‘ì„±, 2-3ë¬¸ì¥ ì´ë‚´
4. êµ¬ì²´ì ì´ê³  í¥ë¯¸ë¡œìš´ ë‚´ìš© ìœ„ì£¼ë¡œ ì‘ì„±

ìš”ì•½:"""

        print("LM Studio í˜¸ì¶œ ì‹œì‘...")
        summary = await call_lm_studio(prompt, max_tokens=200, temperature=0.4)
        print(f"LM Studio ì‘ë‹µ: {summary}")

        # ìš”ì•½ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ì–´ì¸ ê²½ìš° ê¸°ë³¸ ìš”ì•½ ì œê³µ
        if (
            not summary
            or len(summary.strip()) < 10
            or any(word in summary for word in ["Let me", "Based on", "The stream"])
        ):
            summary = f"{title}ì—ì„œ ì§„í–‰ëœ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°ì…ë‹ˆë‹¤. "
            if tags:
                summary += f"ì£¼ìš” ë‚´ìš©ì€ {', '.join(tags[:3])} ê´€ë ¨ì´ë©°, "
            summary += "ì‹œì²­ìë“¤ê³¼ì˜ ì‹¤ì‹œê°„ ì†Œí†µì´ í™œë°œí–ˆìŠµë‹ˆë‹¤."

        return summary.strip()

    except Exception as e:
        print(f"Error generating summary: {str(e)}")
        return f"{title}ì—ì„œ ì§„í–‰ëœ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°ì…ë‹ˆë‹¤."


def extract_highlights_from_comments(comments: List[str], title: str) -> List[str]:
    """ëŒ“ê¸€ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ."""
    if not comments:
        return []

    highlights = []
    highlight_keywords = [
        "ëŒ€ë°•",
        "ìµœê³ ",
        "ì›ƒê²¨",
        "ì¬ë°Œ",
        "ê°ë™",
        "ë†€ë¼",
        "ì‹ ê¸°",
        "ë©‹ì§€",
        "ì™„ë²½",
        "í›Œë¥­",
        "funny",
        "amazing",
        "great",
        "awesome",
        "perfect",
        "incredible",
        "wow",
    ]

    # í•˜ì´ë¼ì´íŠ¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëŒ“ê¸€ ì°¾ê¸°
    for comment in comments[:20]:  # ìƒìœ„ 20ê°œ ëŒ“ê¸€ë§Œ í™•ì¸
        comment_lower = comment.lower()
        if any(keyword in comment_lower for keyword in highlight_keywords):
            # ëŒ“ê¸€ì„ ê°„ë‹¨íˆ ì •ë¦¬í•´ì„œ í•˜ì´ë¼ì´íŠ¸ë¡œ ì¶”ê°€
            clean_comment = comment.strip()[:50]  # 50ì ì œí•œ
            if clean_comment and clean_comment not in highlights:
                highlights.append(clean_comment)
                if len(highlights) >= 3:  # ìµœëŒ€ 3ê°œ
                    break

    # í•˜ì´ë¼ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í•˜ì´ë¼ì´íŠ¸ ìƒì„±
    if not highlights:
        if "ê²Œì„" in title or "game" in title.lower():
            highlights.append("ğŸ® ê²Œì„ ìŠ¤íŠ¸ë¦¬ë°")
        if "ì±„íŒ…" in title or "chat" in title.lower():
            highlights.append("ğŸ’¬ ì‹œì²­ìì™€ ì†Œí†µ")
        if not highlights:
            highlights.append("ğŸ“º ë¼ì´ë¸Œ ë°©ì†¡")

    return highlights


def calculate_engagement_score(view_count: int, like_count: int, comment_count: int, duration_minutes: int) -> float:
    """ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°."""
    if view_count == 0:
        return 0.0

    # ê¸°ë³¸ ì°¸ì—¬ë„ ê³„ì‚° (ì¢‹ì•„ìš”ìœ¨ + ëŒ“ê¸€ìœ¨)
    like_rate = (like_count / view_count) * 100
    comment_rate = (comment_count / view_count) * 100

    # ì‹œê°„ë‹¹ ì°¸ì—¬ë„ ì¡°ì • (ê¸´ ìŠ¤íŠ¸ë¦¼ì¼ìˆ˜ë¡ ì°¸ì—¬ë„ê°€ ë†’ê²Œ í‰ê°€)
    duration_factor = min(duration_minutes / 60, 3)  # ìµœëŒ€ 3ì‹œê°„ê¹Œì§€ë§Œ ë³´ë„ˆìŠ¤

    engagement_score = (like_rate * 0.6 + comment_rate * 0.4) * (1 + duration_factor * 0.1)

    return round(min(engagement_score, 10.0), 2)  # ìµœëŒ€ 10ì 


def categorize_stream(title: str, tags: List[str], keywords: List[str]) -> str:
    """ìŠ¤íŠ¸ë¦¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜."""
    all_text = f"{title} {' '.join(tags)} {' '.join(keywords)}".lower()

    categories = {
        "ğŸ® ê²Œì„": ["ê²Œì„", "game", "í”Œë ˆì´", "play", "rpg", "fps", "moba"],
        "ğŸµ ìŒì•…": ["ìŒì•…", "music", "ë…¸ë˜", "song", "sing", "cover"],
        "ğŸ—£ï¸ í† í¬": ["í† í¬", "talk", "ì±„íŒ…", "chat", "ì†Œí†µ", "qa", "ì§ˆë¬¸"],
        "ğŸ¨ ì°½ì‘": ["ê·¸ë¦¼", "draw", "art", "ì°½ì‘", "ë§Œë“¤ê¸°", "diy"],
        "ğŸ“š êµìœ¡": ["ê°•ì˜", "êµìœ¡", "tutorial", "ë°°ìš°ê¸°", "learn", "study"],
        "ğŸ³ ìš”ë¦¬": ["ìš”ë¦¬", "cook", "ë¨¹ë°©", "food", "recipe"],
        "ğŸƒ ìš´ë™": ["ìš´ë™", "workout", "fitness", "í—¬ìŠ¤", "ìŠ¤í¬ì¸ "],
        "ğŸ¬ ë¦¬ë·°": ["ë¦¬ë·°", "review", "í›„ê¸°", "í‰ê°€", "ë°˜ì‘"],
    }

    for category, category_keywords in categories.items():
        if any(keyword in all_text for keyword in category_keywords):
            return category

    return "ğŸ“º ì¼ë°˜"


def parse_duration_to_minutes(duration: str) -> int:
    """ISO 8601 durationì„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜."""
    if not duration:
        return 0

    import re

    match = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", duration)
    if not match:
        return 0

    hours = int(match.group(1)) if match.group(1) else 0
    minutes = int(match.group(2)) if match.group(2) else 0
    seconds = int(match.group(3)) if match.group(3) else 0

    return hours * 60 + minutes + (seconds // 60)


async def analyze_stream_details(
    entries: List[LiveStreamEntry], settings: Settings, max_videos: int = 20
) -> Dict[str, Any]:
    """YouTube Data APIë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¼ ìƒì„¸ ë¶„ì„."""
    try:
        # YouTube ë¶„ì„ API ì„í¬íŠ¸ (ì—¬ê¸°ì„œ ì„í¬íŠ¸í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from uha.backend.api.youtube_analysis import StreamAnalysisRequest, analyze_streams

        # ë¶„ì„í•  ì˜ìƒ URL ì„ íƒ (ìµœì‹ ìˆœìœ¼ë¡œ)
        video_urls = [entry.url for entry in entries[:max_videos]]

        # YouTube ë¶„ì„ ìš”ì²­
        analysis_request = StreamAnalysisRequest(video_urls=video_urls, extract_comments=True, max_comments=20)

        # ë¶„ì„ ì‹¤í–‰
        analysis_result = await analyze_streams(analysis_request, settings)

        return {
            "videos": [video.dict() for video in analysis_result.videos],
            "summary": analysis_result.summary,
            "common_keywords": analysis_result.common_keywords,
            "total_engagement": analysis_result.total_engagement,
        }

    except Exception as e:
        print(f"YouTube ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {"error": f"YouTube ë¶„ì„ ì‹¤íŒ¨: {str(e)}", "videos": [], "common_keywords": [], "total_engagement": {}}


@router.post("/summarize", response_model=SummaryResponse)
async def summarize_text(request: SummaryRequest):
    """Summarize given text using LM Studio."""
    prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{request.content}"

    summary = await call_lm_studio(prompt, request.max_tokens, request.temperature)

    return SummaryResponse(
        summary=summary.strip(), original_length=len(request.content), summary_length=len(summary.strip())
    )


@router.post("/summarize-live-streams", response_model=LiveStreamSummaryResponse)
async def summarize_live_streams(request: LiveStreamSummaryRequest):
    """Summarize live stream data for a specific year."""
    settings = await get_settings()

    # Fetch live stream data
    content = await fetch_live_stream_data(request.year)

    # Parse the data
    entries = parse_live_stream_data(content, request.date_filter)

    if not entries:
        raise HTTPException(status_code=404, detail=f"{request.year}ë…„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # YouTube ìƒì„¸ ë¶„ì„ ìˆ˜í–‰ (ì˜µì…˜)
    detailed_analysis = None
    common_keywords = None
    engagement_stats = None

    if request.include_detailed_analysis and settings.youtube_api_key:
        print(f"YouTube ìƒì„¸ ë¶„ì„ ì‹œì‘: {min(len(entries), request.max_videos_to_analyze)}ê°œ ì˜ìƒ")
        detailed_analysis = await analyze_stream_details(entries, settings, request.max_videos_to_analyze)
        common_keywords = detailed_analysis.get("common_keywords", [])
        engagement_stats = detailed_analysis.get("total_engagement", {})

    # Create summary prompt
    dates_only = [entry.date for entry in entries[:20]]  # Limit to first 20 entries for analysis
    dates_text = ", ".join(dates_only)

    # ìƒì„¸ ë¶„ì„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    additional_info = ""
    if detailed_analysis and detailed_analysis.get("videos"):
        video_count = len(detailed_analysis["videos"])
        total_views = engagement_stats.get("total_views", 0)
        total_likes = engagement_stats.get("total_likes", 0)
        keywords_text = ", ".join(common_keywords[:10]) if common_keywords else ""

        additional_info = f"""

ì¶”ê°€ ë¶„ì„ ë°ì´í„°:
- ë¶„ì„ëœ ì˜ìƒ ìˆ˜: {video_count}ê°œ
- ì´ ì¡°íšŒìˆ˜: {total_views:,}íšŒ
- ì´ ì¢‹ì•„ìš”: {total_likes:,}ê°œ
- ì£¼ìš” í‚¤ì›Œë“œ: {keywords_text}
"""

    prompt = f"""
{request.year}ë…„ì— ì´ {len(entries)}íšŒì˜ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ìš” ë‚ ì§œëŠ” {dates_text} ë“±ì…ë‹ˆë‹¤.
{additional_info}

ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ í•œêµ­ì–´ë¡œ 3-4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ì›”ë³„ í™œë™ëŸ‰ê³¼ íŒ¨í„´
2. ê°€ì¥ í™œë°œí–ˆë˜ ì‹œê¸°
3. ì „ì²´ì ì¸ ìŠ¤íŠ¸ë¦¬ë° íŠ¹ì§•
4. ì‹œì²­ì ë°˜ì‘ ë° ì°¸ì—¬ë„ (ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)

ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ê³  êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""

    try:
        summary = await call_lm_studio(prompt, max_tokens=500, temperature=0.3)

        # If the summary is in English or contains thinking patterns, provide a fallback
        if not summary or "Okay" in summary or "Let me" in summary or len(summary.strip()) < 10:
            # Provide a simple Korean summary based on the data
            total_count = len(entries)
            first_date = entries[0].date if entries else ""
            last_date = entries[-1].date if entries else ""

            base_summary = f"{request.year}ë…„ì— ì´ {total_count}íšŒì˜ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. í™œë™ ê¸°ê°„ì€ {last_date}ë¶€í„° {first_date}ê¹Œì§€ì´ë©°, ê¾¸ì¤€í•œ ë°©ì†¡ í™œë™ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."

            if engagement_stats:
                avg_views = engagement_stats.get("average_views", 0)
                total_likes = engagement_stats.get("total_likes", 0)
                base_summary += (
                    f" ë¶„ì„ëœ ì˜ìƒë“¤ì˜ í‰ê·  ì¡°íšŒìˆ˜ëŠ” {avg_views:,}íšŒì´ë©°, ì´ {total_likes:,}ê°œì˜ ì¢‹ì•„ìš”ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤."
                )

            summary = base_summary

    except Exception:
        # Fallback summary if LM Studio fails
        total_count = len(entries)
        summary = f"{request.year}ë…„ì— ì´ {total_count}íšŒì˜ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ë°©ì†¡ í™œë™ì„ í†µí•´ ì‹œì²­ìë“¤ê³¼ ê¾¸ì¤€íˆ ì†Œí†µí–ˆìŠµë‹ˆë‹¤."

    return LiveStreamSummaryResponse(
        entries=entries,
        summary=summary.strip(),
        total_streams=len(entries),
        detailed_analysis=detailed_analysis,
        common_keywords=common_keywords,
        engagement_stats=engagement_stats,
    )


@router.post("/streams", response_model=PaginatedStreamsResponse)
async def get_paginated_streams(request: PaginatedStreamsRequest):
    """Get paginated live streams with detailed information."""
    settings = await get_settings()

    # Fetch live stream data
    content = await fetch_live_stream_data(request.year)

    # Parse the data
    entries = parse_live_stream_data(content)

    if not entries:
        return PaginatedStreamsResponse(
            streams=[], total_streams=0, current_page=request.page, total_pages=0, per_page=request.per_page
        )

    # Calculate pagination
    total_streams = len(entries)
    total_pages = (total_streams + request.per_page - 1) // request.per_page
    start_idx = (request.page - 1) * request.per_page
    end_idx = start_idx + request.per_page
    paginated_entries = entries[start_idx:end_idx]

    # Get detailed information for each stream
    streams_with_details = []
    print(
        f"ìƒì„¸ ì •ë³´ ì²˜ë¦¬ ì‹œì‘ - include_details: {request.include_details}, youtube_api_key ì¡´ì¬: {bool(settings.youtube_api_key)}"
    )

    if request.include_details:
        print(f"ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°: {len(paginated_entries)}ê°œ ìŠ¤íŠ¸ë¦¼")
        # ë™ì‹œì— ì—¬ëŸ¬ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œì”©)
        import asyncio

        async def process_batch(batch_entries):
            tasks = [get_stream_details(entry, settings) for entry in batch_entries]
            return await asyncio.gather(*tasks, return_exceptions=True)

        # 5ê°œì”© ë°°ì¹˜ë¡œ ì²˜ë¦¬
        for i in range(0, len(paginated_entries), 5):
            batch = paginated_entries[i : i + 5]
            batch_results = await process_batch(batch)

            for result in batch_results:
                if not isinstance(result, Exception):
                    streams_with_details.append(result)
                else:
                    print(f"Error processing stream: {result}")
    else:
        print("ìƒì„¸ ì •ë³´ ì—†ì´ ê¸°ë³¸ ì •ë³´ë§Œ ì²˜ë¦¬")
        # ìƒì„¸ ì •ë³´ ì—†ì´ ê¸°ë³¸ ì •ë³´ë§Œ
        for entry in paginated_entries:
            video_id = extract_video_id_from_url(entry.url)
            streams_with_details.append(StreamWithDetails(date=entry.date, url=entry.url, video_id=video_id))

    return PaginatedStreamsResponse(
        streams=streams_with_details,
        total_streams=total_streams,
        current_page=request.page,
        total_pages=total_pages,
        per_page=request.per_page,
    )


@router.get("/health")
async def check_lm_studio_health():
    """Check if LM Studio is running and accessible."""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:1234/v1/models")

        if response.status_code == 200:
            return {"status": "healthy", "message": "LM Studio is running"}
        else:
            return {"status": "unhealthy", "message": f"LM Studio returned status {response.status_code}"}

    except httpx.ConnectError:
        return {"status": "unhealthy", "message": "Cannot connect to LM Studio"}
    except Exception as e:
        return {"status": "unhealthy", "message": f"Error: {str(e)}"}


@router.post("/cache/clear")
async def clear_cache() -> dict:
    """Clear expired cache entries."""
    try:
        deleted_count = await cache_service.clear_expired_cache()
        return {"status": "success", "message": f"Cleared {deleted_count} expired cache entries"}
    except Exception as e:
        return {"status": "error", "message": f"Failed to clear cache: {str(e)}"}


@router.get("/cache/stats")
async def cache_stats() -> dict:
    """Get cache statistics."""
    try:
        # Get cache statistics for recent years
        stats = {}
        for year in [2020, 2021, 2022, 2023, 2024, 2025]:
            count = await cache_service.get_cached_stream_count(year)
            if count > 0:
                stats[str(year)] = count

        return {"status": "success", "cached_streams_by_year": stats, "total_cached_streams": sum(stats.values())}
    except Exception as e:
        return {"status": "error", "message": f"Failed to get cache stats: {str(e)}"}
