"""LLM integration API endpoints."""

from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

from uha.backend.container import ApplicationContainer
from uha.backend.database.models import DatabaseManager
from uha.backend.models.stream_models import (
    LiveStreamEntry,
    PaginatedStreamsRequest,
    PaginatedStreamsResponse,
    StreamWithDetails,
)
from uha.backend.services.cache_service import StreamCacheService
from uha.backend.settings import Settings

router = APIRouter(prefix="/llm", tags=["LLM"])

# Initialize database and cache service
db_manager = DatabaseManager()
cache_service = StreamCacheService(db_manager)


class SummaryRequest(BaseModel):
    content: str
    max_tokens: Optional[int] = 500
    temperature: Optional[float] = 0.7


class SummaryResponse(BaseModel):
    summary: str
    original_length: int
    summary_length: int


class LiveStreamSummaryRequest(BaseModel):
    year: int
    date_filter: Optional[str] = None  # YYYY-MM-DD format
    include_detailed_analysis: bool = False  # Include YouTube Data API detailed analysis
    max_videos_to_analyze: int = 20  # Maximum number of videos to analyze


class LiveStreamSummaryResponse(BaseModel):
    entries: List[LiveStreamEntry]
    summary: str
    total_streams: int
    detailed_analysis: Optional[Dict[str, Any]] = None  # YouTube Data API analysis results
    common_keywords: Optional[List[str]] = None  # ê³µí†µ í‚¤ì›Œë“œ
    engagement_stats: Optional[Dict[str, int]] = None  # ì°¸ì—¬ë„ í†µê³„


async def get_settings() -> Settings:
    """Get application settings."""
    container = ApplicationContainer()
    return container.settings.provided()


async def call_lm_studio(prompt: str, max_tokens: int = 500, temperature: float = 0.3) -> str:
    """Call LM Studio API for text generation."""
    url = "http://localhost:1234/v1/chat/completions"

    payload = {
        "model": "qwen/qwen3-4b",
        "messages": [
            {
                "role": "system",
                "content": "You are a Korean language specialist. Respond ONLY in Korean. Provide concise 2-3 sentence summaries.",
            },
            {"role": "user", "content": prompt},
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stop": ["\n\n", "Summary:"],
    }

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, json=payload)

        if response.status_code != 200:
            raise HTTPException(status_code=500, detail=f"LM Studio API call failed: {response.status_code}")

        data = response.json()
        content = data["choices"][0]["message"]["content"]

        # Clean up AI response - remove thinking tags and extra content
        content = content.strip()
        if content.startswith("<think>"):
            # Find the end of thinking section and extract only the actual response
            think_end = content.find("</think>")
            if think_end != -1:
                content = content[think_end + 8 :].strip()

        # Remove any remaining XML-like tags
        import re

        content = re.sub(r"<[^>]+>", "", content).strip()

        # Take only the first few sentences for summary
        sentences = content.split(".")
        if len(sentences) > 4:
            content = ". ".join(sentences[:4]) + "."

        return content

    except httpx.TimeoutException:
        raise HTTPException(status_code=504, detail="LM Studio API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
    except httpx.ConnectError:
        raise HTTPException(
            status_code=503,
            detail="LM Studio ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. http://localhost:1234 ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LM Studio API ì˜¤ë¥˜: {str(e)}")


async def fetch_live_stream_data(year: int) -> str:
    """Fetch live stream data from local submodule."""
    import os

    # Try data/vendor directory first (new location), then fallback to old locations
    possible_paths = [
        f"data/vendor/uzuhama-live-link/readme-{year}.md",
        f"vendor/uzuhama-live-link/readme-{year}.md",
        f"uzuhama-live-link/readme-{year}.md",
    ]

    for file_path in possible_paths:
        if os.path.exists(file_path):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    return f.read()
            except Exception:
                continue

    # Fallback to GitHub if local files not found
    url = f"https://raw.githubusercontent.com/eun2ce/uzuhama-live-link/main/readme-{year}.md"

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url)

        if response.status_code != 200:
            raise HTTPException(status_code=404, detail=f"{year}ë…„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        return response.text

    except httpx.ConnectError:
        raise HTTPException(status_code=503, detail="ë¡œì»¬ íŒŒì¼ê³¼ GitHub ì €ì¥ì†Œ ëª¨ë‘ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(e)}")


def parse_live_stream_data(content: str, date_filter: Optional[str] = None) -> List[LiveStreamEntry]:
    """Parse live stream data from markdown content."""
    entries = []
    lines = content.strip().split("\n")

    for line in lines:
        line = line.strip()

        # Skip header lines and separators
        if line.startswith("|") and "Date" not in line and "---" not in line:
            # Parse markdown table format: | Date | URL |
            parts = [part.strip() for part in line.split("|") if part.strip()]
            if len(parts) >= 2:
                date = parts[0].strip()
                url_part = parts[1].strip()

                # Extract URL from markdown link format [text](url)
                if url_part.startswith("[") and "](" in url_part and url_part.endswith(")"):
                    url = url_part.split("](")[1][:-1]  # Extract URL from [text](url)
                else:
                    url = url_part

                # Apply date filter if provided
                if date_filter and not date.startswith(date_filter):
                    continue

                entries.append(LiveStreamEntry(date=date, url=url))

        # Also handle tab-separated format for backward compatibility
        elif "\t" in line:
            parts = line.split("\t")
            if len(parts) >= 2:
                date = parts[0].strip()
                url = parts[1].strip()

                # Apply date filter if provided
                if date_filter and not date.startswith(date_filter):
                    continue

                entries.append(LiveStreamEntry(date=date, url=url))

    return entries


def extract_video_id_from_url(url: str) -> str:
    """Extract video ID from YouTube URL."""
    import re

    patterns = [
        r"(?:youtube\.com\/watch\?v=|youtu\.be\/)([a-zA-Z0-9_-]{11})",
        r"youtube\.com\/embed\/([a-zA-Z0-9_-]{11})",
        r"youtube\.com\/v\/([a-zA-Z0-9_-]{11})",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return ""


async def get_stream_details(entry: LiveStreamEntry, settings: Settings) -> StreamWithDetails:
    """Get detailed information for a single stream with caching."""
    # Get stream details
    video_id = extract_video_id_from_url(entry.url)
    # Extract video ID

    if not video_id:
        # No video ID found
        return StreamWithDetails(
            date=entry.date,
            url=entry.url,
            video_id=video_id,
        )

    # Try to get from cache first
    cached_stream = await cache_service.get_cached_stream(video_id)
    if cached_stream:
        return cached_stream

    stream_details = StreamWithDetails(
        date=entry.date,
        url=entry.url,
        video_id=video_id,
    )

    print(
        f"YouTube API í‚¤ ìƒíƒœ: {bool(settings.youtube_api_key)} (ê¸¸ì´: {len(settings.youtube_api_key) if settings.youtube_api_key else 0})"
    )

    if not settings.youtube_api_key:
        # No YouTube API key - using dummy data
        # ë”ë¯¸ ë°ì´í„°ë¡œ AI ìš”ì•½ í…ŒìŠ¤íŠ¸
        stream_details.title = f"í…ŒìŠ¤íŠ¸ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ - {entry.date}"
        # YouTube ì¸ë„¤ì¼ URL íŒ¨í„´ ì‚¬ìš©
        stream_details.thumbnail = f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
        stream_details.ai_summary = await generate_stream_summary(
            stream_details.title,
            "í…ŒìŠ¤íŠ¸ìš© ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì…ë‹ˆë‹¤. ì‹œì²­ìë“¤ê³¼ í•¨ê»˜ ì¦ê±°ìš´ ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤.",
            ["ì¬ë°Œì–´ìš”!", "ëŒ€ë°•!", "ë‹¤ìŒì—ë„ í•´ì£¼ì„¸ìš”"],
            ["ê²Œì„", "ë¼ì´ë¸Œ"],
            ["ìŠ¤íŠ¸ë¦¬ë°", "ì¬ë¯¸"],
        )
        stream_details.highlights = ["ğŸ® ê²Œì„ ìŠ¤íŠ¸ë¦¬ë°", "ğŸ’¬ ì‹œì²­ìì™€ ì†Œí†µ"]
        stream_details.sentiment = "ê¸ì •ì ì¸ ë°˜ì‘ì´ ë§ì€ ìŠ¤íŠ¸ë¦¼"
        stream_details.engagement_score = 7.5
        stream_details.category = "ğŸ® ê²Œì„"
        stream_details.view_count = 1500
        stream_details.like_count = 120
        stream_details.comment_count = 45
        stream_details.duration = "PT1H30M"

        # Cache the dummy data
        await cache_service.cache_stream(stream_details)
        return stream_details

    try:
        # YouTube Data API í˜¸ì¶œ
        from uha.backend.api.youtube_analysis import extract_keywords_from_text, get_video_comments, get_video_details

        video_data = await get_video_details(video_id, settings.youtube_api_key)
        snippet = video_data["snippet"]
        statistics = video_data["statistics"]
        content_details = video_data["contentDetails"]

        # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
        comments_data = await get_video_comments(video_id, settings.youtube_api_key, max_results=20)
        comment_texts = []
        for comment_item in comments_data:
            comment = comment_item["snippet"]["topLevelComment"]["snippet"]
            comment_texts.append(comment["textDisplay"])

        # ì¸ë„¤ì¼ URL
        thumbnails = snippet.get("thumbnails", {})
        thumbnail_url = (
            thumbnails.get("maxres", {}).get("url")
            or thumbnails.get("high", {}).get("url")
            or thumbnails.get("medium", {}).get("url")
            or thumbnails.get("default", {}).get("url")
        )

        # í‚¤ì›Œë“œ ì¶”ì¶œ
        text_for_keywords = f"{snippet['title']} {snippet.get('description', '')}"
        keywords = extract_keywords_from_text(text_for_keywords, max_keywords=5)

        # ê¸°ë³¸ ì •ë³´ ì„¤ì •
        stream_details.title = snippet["title"]
        stream_details.thumbnail = thumbnail_url
        stream_details.view_count = int(statistics.get("viewCount", 0))
        stream_details.like_count = int(statistics.get("likeCount", 0))
        stream_details.comment_count = int(statistics.get("commentCount", 0))
        stream_details.duration = content_details.get("duration", "")
        stream_details.tags = snippet.get("tags", [])[:5]  # ìµœëŒ€ 5ê°œ íƒœê·¸
        stream_details.keywords = keywords

        # ì¶”ê°€ ë¶„ì„ ì •ë³´ ìƒì„±
        try:
            # Start AI analysis
            # AI ìš”ì•½ ìƒì„±
            stream_details.ai_summary = await generate_stream_summary(
                snippet["title"], snippet.get("description", ""), comment_texts, stream_details.tags or [], keywords
            )
            # AI summary completed

            # í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ
            stream_details.highlights = extract_highlights_from_comments(comment_texts, snippet["title"])

            # ê°ì • ë¶„ì„
            from uha.backend.api.youtube_analysis import analyze_video_sentiment

            stream_details.sentiment = await analyze_video_sentiment(
                snippet["title"], snippet.get("description", ""), comment_texts
            )

            # ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°
            duration_minutes = parse_duration_to_minutes(stream_details.duration)
            stream_details.engagement_score = calculate_engagement_score(
                stream_details.view_count, stream_details.like_count, stream_details.comment_count, duration_minutes
            )

            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            stream_details.category = categorize_stream(snippet["title"], stream_details.tags or [], keywords)

        except Exception as analysis_error:
            print(f"Error in additional analysis for {video_id}: {str(analysis_error)}")
            # ê¸°ë³¸ê°’ ì„¤ì •
            stream_details.ai_summary = f"{snippet['title']}ì—ì„œ ì§„í–‰ëœ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°ì…ë‹ˆë‹¤."
            stream_details.highlights = ["ğŸ“º ë¼ì´ë¸Œ ë°©ì†¡"]
            stream_details.sentiment = "ì¤‘ë¦½ì ì¸ ë°˜ì‘ì˜ ìŠ¤íŠ¸ë¦¼"
            stream_details.engagement_score = 0.0
            stream_details.category = "ğŸ“º ì¼ë°˜"

    except Exception as e:
        print(f"Error getting details for {entry.url}: {str(e)}")

    # Cache the processed stream data
    await cache_service.cache_stream(stream_details)
    return stream_details


async def generate_stream_summary(
    title: str, description: str, comments: List[str], tags: List[str], keywords: List[str]
) -> str:
    """Generate AI summary for individual stream."""
    try:
        # Generate summary function called

        # Extract main content from comments (top 10)
        top_comments = comments[:10]
        comments_text = " ".join(top_comments) if top_comments else ""

        # Combine tags and keywords
        all_tags = ", ".join(tags + keywords) if tags or keywords else ""

        prompt = f"""
Please write a concise 2-3 sentence summary in Korean based on the following live stream information:

Title: {title}
Description: {description[:200]}...
Main tags/keywords: {all_tags}
Viewer comments summary: {comments_text[:300]}...

Summary conditions:
1. Briefly describe the main content and features of the stream
2. Include viewer reactions or highlights if available
3. Write only in Korean, within 2-3 sentences
4. Focus on specific and interesting content

Summary:"""

        # Call LM Studio
        summary = await call_lm_studio(prompt, max_tokens=200, temperature=0.4)
        # LM Studio response received

        # Provide default summary if too short or in English
        if (
            not summary
            or len(summary.strip()) < 10
            or any(word in summary for word in ["Let me", "Based on", "The stream"])
        ):
            summary = f"Live streaming session from {title}. "
            if tags:
                summary += f"Main content related to {', '.join(tags[:3])}, "
            summary += "Active real-time communication with viewers."

        return summary.strip()

    except Exception as e:
        print(f"Error generating summary: {str(e)}")
        return f"Live streaming session from {title}."


def extract_highlights_from_comments(comments: List[str], title: str) -> List[str]:
    """ëŒ“ê¸€ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ì¶”ì¶œ."""
    if not comments:
        return []

    highlights = []
    highlight_keywords = [
        "ëŒ€ë°•",
        "ìµœê³ ",
        "ì›ƒê²¨",
        "ì¬ë°Œ",
        "ê°ë™",
        "ë†€ë¼",
        "ì‹ ê¸°",
        "ë©‹ì§€",
        "ì™„ë²½",
        "í›Œë¥­",
        "funny",
        "amazing",
        "great",
        "awesome",
        "perfect",
        "incredible",
        "wow",
    ]

    # í•˜ì´ë¼ì´íŠ¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëŒ“ê¸€ ì°¾ê¸°
    for comment in comments[:20]:  # ìƒìœ„ 20ê°œ ëŒ“ê¸€ë§Œ í™•ì¸
        comment_lower = comment.lower()
        if any(keyword in comment_lower for keyword in highlight_keywords):
            # ëŒ“ê¸€ì„ ê°„ë‹¨íˆ ì •ë¦¬í•´ì„œ í•˜ì´ë¼ì´íŠ¸ë¡œ ì¶”ê°€
            clean_comment = comment.strip()[:50]  # 50ì ì œí•œ
            if clean_comment and clean_comment not in highlights:
                highlights.append(clean_comment)
                if len(highlights) >= 3:  # ìµœëŒ€ 3ê°œ
                    break

    # í•˜ì´ë¼ì´íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í•˜ì´ë¼ì´íŠ¸ ìƒì„±
    if not highlights:
        if "ê²Œì„" in title or "game" in title.lower():
            highlights.append("ğŸ® ê²Œì„ ìŠ¤íŠ¸ë¦¬ë°")
        if "ì±„íŒ…" in title or "chat" in title.lower():
            highlights.append("ğŸ’¬ ì‹œì²­ìì™€ ì†Œí†µ")
        if not highlights:
            highlights.append("ğŸ“º ë¼ì´ë¸Œ ë°©ì†¡")

    return highlights


def calculate_engagement_score(view_count: int, like_count: int, comment_count: int, duration_minutes: int) -> float:
    """ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°."""
    if view_count == 0:
        return 0.0

    # ê¸°ë³¸ ì°¸ì—¬ë„ ê³„ì‚° (ì¢‹ì•„ìš”ìœ¨ + ëŒ“ê¸€ìœ¨)
    like_rate = (like_count / view_count) * 100
    comment_rate = (comment_count / view_count) * 100

    # ì‹œê°„ë‹¹ ì°¸ì—¬ë„ ì¡°ì • (ê¸´ ìŠ¤íŠ¸ë¦¼ì¼ìˆ˜ë¡ ì°¸ì—¬ë„ê°€ ë†’ê²Œ í‰ê°€)
    duration_factor = min(duration_minutes / 60, 3)  # ìµœëŒ€ 3ì‹œê°„ê¹Œì§€ë§Œ ë³´ë„ˆìŠ¤

    engagement_score = (like_rate * 0.6 + comment_rate * 0.4) * (1 + duration_factor * 0.1)

    return round(min(engagement_score, 10.0), 2)  # ìµœëŒ€ 10ì 


def categorize_stream(title: str, tags: List[str], keywords: List[str]) -> str:
    """ìŠ¤íŠ¸ë¦¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜."""
    all_text = f"{title} {' '.join(tags)} {' '.join(keywords)}".lower()

    categories = {
        "ğŸ® ê²Œì„": ["ê²Œì„", "game", "í”Œë ˆì´", "play", "rpg", "fps", "moba"],
        "ğŸµ ìŒì•…": ["ìŒì•…", "music", "ë…¸ë˜", "song", "sing", "cover"],
        "ğŸ—£ï¸ í† í¬": ["í† í¬", "talk", "ì±„íŒ…", "chat", "ì†Œí†µ", "qa", "ì§ˆë¬¸"],
        "ğŸ¨ ì°½ì‘": ["ê·¸ë¦¼", "draw", "art", "ì°½ì‘", "ë§Œë“¤ê¸°", "diy"],
        "ğŸ“š êµìœ¡": ["ê°•ì˜", "êµìœ¡", "tutorial", "ë°°ìš°ê¸°", "learn", "study"],
        "ğŸ³ ìš”ë¦¬": ["ìš”ë¦¬", "cook", "ë¨¹ë°©", "food", "recipe"],
        "ğŸƒ ìš´ë™": ["ìš´ë™", "workout", "fitness", "í—¬ìŠ¤", "ìŠ¤í¬ì¸ "],
        "ğŸ¬ ë¦¬ë·°": ["ë¦¬ë·°", "review", "í›„ê¸°", "í‰ê°€", "ë°˜ì‘"],
    }

    for category, category_keywords in categories.items():
        if any(keyword in all_text for keyword in category_keywords):
            return category

    return "ğŸ“º ì¼ë°˜"


def parse_duration_to_minutes(duration: str) -> int:
    """ISO 8601 durationì„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜."""
    if not duration:
        return 0

    import re

    match = re.match(r"PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?", duration)
    if not match:
        return 0

    hours = int(match.group(1)) if match.group(1) else 0
    minutes = int(match.group(2)) if match.group(2) else 0
    seconds = int(match.group(3)) if match.group(3) else 0

    return hours * 60 + minutes + (seconds // 60)


async def analyze_stream_details(
    entries: List[LiveStreamEntry], settings: Settings, max_videos: int = 20
) -> Dict[str, Any]:
    """YouTube Data APIë¥¼ í™œìš©í•œ ìŠ¤íŠ¸ë¦¼ ìƒì„¸ ë¶„ì„."""
    try:
        # YouTube ë¶„ì„ API ì„í¬íŠ¸ (ì—¬ê¸°ì„œ ì„í¬íŠ¸í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from uha.backend.api.youtube_analysis import StreamAnalysisRequest, analyze_streams

        # ë¶„ì„í•  ì˜ìƒ URL ì„ íƒ (ìµœì‹ ìˆœìœ¼ë¡œ)
        video_urls = [entry.url for entry in entries[:max_videos]]

        # YouTube ë¶„ì„ ìš”ì²­
        analysis_request = StreamAnalysisRequest(video_urls=video_urls, extract_comments=True, max_comments=20)

        # ë¶„ì„ ì‹¤í–‰
        analysis_result = await analyze_streams(analysis_request, settings)

        return {
            "videos": [video.dict() for video in analysis_result.videos],
            "summary": analysis_result.summary,
            "common_keywords": analysis_result.common_keywords,
            "total_engagement": analysis_result.total_engagement,
        }

    except Exception as e:
        # YouTube analysis error
        return {
            "error": f"YouTube analysis failed: {str(e)}",
            "videos": [],
            "common_keywords": [],
            "total_engagement": {},
        }


@router.post("/summarize", response_model=SummaryResponse)
async def summarize_text(request: SummaryRequest):
    """Summarize given text using LM Studio."""
    prompt = f"Please summarize the following text in 2-3 concise sentences:\n\n{request.content}"

    summary = await call_lm_studio(prompt, request.max_tokens, request.temperature)

    return SummaryResponse(
        summary=summary.strip(), original_length=len(request.content), summary_length=len(summary.strip())
    )


@router.post("/summarize-live-streams", response_model=LiveStreamSummaryResponse)
async def summarize_live_streams(request: LiveStreamSummaryRequest):
    """Summarize live stream data for a specific year."""
    settings = await get_settings()

    # Fetch live stream data
    content = await fetch_live_stream_data(request.year)

    # Parse the data
    entries = parse_live_stream_data(content, request.date_filter)

    if not entries:
        raise HTTPException(status_code=404, detail=f"{request.year}ë…„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # YouTube ìƒì„¸ ë¶„ì„ ìˆ˜í–‰ (ì˜µì…˜)
    detailed_analysis = None
    common_keywords = None
    engagement_stats = None

    if request.include_detailed_analysis and settings.youtube_api_key:
        # Start YouTube detailed analysis
        detailed_analysis = await analyze_stream_details(entries, settings, request.max_videos_to_analyze)
        common_keywords = detailed_analysis.get("common_keywords", [])
        engagement_stats = detailed_analysis.get("total_engagement", {})

    # Create summary prompt
    dates_only = [entry.date for entry in entries[:20]]  # Limit to first 20 entries for analysis
    dates_text = ", ".join(dates_only)

    # Include detailed analysis data in prompt if available
    additional_info = ""
    if detailed_analysis and detailed_analysis.get("videos"):
        video_count = len(detailed_analysis["videos"])
        total_views = engagement_stats.get("total_views", 0)
        total_likes = engagement_stats.get("total_likes", 0)
        keywords_text = ", ".join(common_keywords[:10]) if common_keywords else ""

        additional_info = f"""

Additional analysis data:
- Analyzed videos: {video_count}
- ì´ ì¡°íšŒìˆ˜: {total_views:,}íšŒ
- ì´ ì¢‹ì•„ìš”: {total_likes:,}ê°œ
- ì£¼ìš” í‚¤ì›Œë“œ: {keywords_text}
"""

    prompt = f"""
{request.year}ë…„ì— ì´ {len(entries)}íšŒì˜ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ìš” ë‚ ì§œëŠ” {dates_text} ë“±ì…ë‹ˆë‹¤.
{additional_info}

Based on this data, please provide a 3-4 sentence summary in Korean:
1. ì›”ë³„ í™œë™ëŸ‰ê³¼ íŒ¨í„´
2. ê°€ì¥ í™œë°œí–ˆë˜ ì‹œê¸°
3. ì „ì²´ì ì¸ ìŠ¤íŠ¸ë¦¬ë° íŠ¹ì§•
4. ì‹œì²­ì ë°˜ì‘ ë° ì°¸ì—¬ë„ (ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)

ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ê³  êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
"""

    try:
        summary = await call_lm_studio(prompt, max_tokens=500, temperature=0.3)

        # If the summary is in English or contains thinking patterns, provide a fallback
        if not summary or "Okay" in summary or "Let me" in summary or len(summary.strip()) < 10:
            # Provide a simple Korean summary based on the data
            total_count = len(entries)
            first_date = entries[0].date if entries else ""
            last_date = entries[-1].date if entries else ""

            base_summary = f"{request.year}ë…„ì— ì´ {total_count}íšŒì˜ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. í™œë™ ê¸°ê°„ì€ {last_date}ë¶€í„° {first_date}ê¹Œì§€ì´ë©°, ê¾¸ì¤€í•œ ë°©ì†¡ í™œë™ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤."

            if engagement_stats:
                avg_views = engagement_stats.get("average_views", 0)
                total_likes = engagement_stats.get("total_likes", 0)
                base_summary += f" Analyzed videos averaged {avg_views:,} views with total {total_likes:,} likes."

            summary = base_summary

    except Exception:
        # Fallback summary if LM Studio fails
        total_count = len(entries)
        summary = f"{request.year}ë…„ì— ì´ {total_count}íšŒì˜ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ë°©ì†¡ í™œë™ì„ í†µí•´ ì‹œì²­ìë“¤ê³¼ ê¾¸ì¤€íˆ ì†Œí†µí–ˆìŠµë‹ˆë‹¤."

    return LiveStreamSummaryResponse(
        entries=entries,
        summary=summary.strip(),
        total_streams=len(entries),
        detailed_analysis=detailed_analysis,
        common_keywords=common_keywords,
        engagement_stats=engagement_stats,
    )


@router.post("/streams", response_model=PaginatedStreamsResponse)
async def get_paginated_streams(request: PaginatedStreamsRequest):
    """Get paginated live streams with detailed information."""
    settings = await get_settings()

    # Fetch live stream data
    content = await fetch_live_stream_data(request.year)

    # Parse the data
    entries = parse_live_stream_data(content)

    if not entries:
        return PaginatedStreamsResponse(
            streams=[], total_streams=0, current_page=request.page, total_pages=0, per_page=request.per_page
        )

    # Calculate pagination
    total_streams = len(entries)
    total_pages = (total_streams + request.per_page - 1) // request.per_page
    start_idx = (request.page - 1) * request.per_page
    end_idx = start_idx + request.per_page
    paginated_entries = entries[start_idx:end_idx]

    # Get detailed information for each stream
    streams_with_details = []

    if request.include_details:
        # Process multiple streams concurrently (max 5 at a time)
        import asyncio

        async def process_batch(batch_entries):
            tasks = [get_stream_details(entry, settings) for entry in batch_entries]
            return await asyncio.gather(*tasks, return_exceptions=True)

        # Process in batches of 5
        for i in range(0, len(paginated_entries), 5):
            batch = paginated_entries[i : i + 5]
            batch_results = await process_batch(batch)

            for result in batch_results:
                if not isinstance(result, Exception):
                    streams_with_details.append(result)
                else:
                    print(f"Error processing stream: {result}")
    else:
        # Processing basic info only
        # ìƒì„¸ ì •ë³´ ì—†ì´ ê¸°ë³¸ ì •ë³´ë§Œ
        for entry in paginated_entries:
            video_id = extract_video_id_from_url(entry.url)
            streams_with_details.append(StreamWithDetails(date=entry.date, url=entry.url, video_id=video_id))

    return PaginatedStreamsResponse(
        streams=streams_with_details,
        total_streams=total_streams,
        current_page=request.page,
        total_pages=total_pages,
        per_page=request.per_page,
    )


@router.get("/health")
async def check_lm_studio_health():
    """Check if LM Studio is running and accessible."""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:1234/v1/models")

        if response.status_code == 200:
            return {"status": "healthy", "message": "LM Studio is running"}
        else:
            return {"status": "unhealthy", "message": f"LM Studio returned status {response.status_code}"}

    except httpx.ConnectError:
        return {"status": "unhealthy", "message": "Cannot connect to LM Studio"}
    except Exception as e:
        return {"status": "unhealthy", "message": f"Error: {str(e)}"}


@router.post("/cache/clear")
async def clear_cache() -> dict:
    """Clear expired cache entries."""
    try:
        deleted_count = await cache_service.clear_expired_cache()
        return {"status": "success", "message": f"Cleared {deleted_count} expired cache entries"}
    except Exception as e:
        return {"status": "error", "message": f"Failed to clear cache: {str(e)}"}


@router.get("/cache/stats")
async def cache_stats() -> dict:
    """Get cache statistics."""
    try:
        # Get cache statistics for recent years
        stats = {}
        for year in [2020, 2021, 2022, 2023, 2024, 2025]:
            count = await cache_service.get_cached_stream_count(year)
            if count > 0:
                stats[str(year)] = count

        return {"status": "success", "cached_streams_by_year": stats, "total_cached_streams": sum(stats.values())}
    except Exception as e:
        return {"status": "error", "message": f"Failed to get cache stats: {str(e)}"}
